---
layout: post
title: Support Vector Machine - SVM
---

# Introducci√≥n

En esta primera entrada del blog sobre ciencia de datos y machien learning, vamos a tratar de explicar el algoritmo SVM (Support Vector Machines) orientado a tareas de clasificaci√≥n binarias. Se tratar√° de aportar una explicaci√≥n sencilla a la par de rigurosa, comenzando por dar una idea sencilla y aproximada de que es y que hace este algoritmo, para seguir con la explicaci√≥n del SVM lineal, implementando este algoritmo 'from scratch' y finalizaremos el post explicando como lidiar con conjuntos de datos no lineales. ¬°Espero que os guste!

# Indice



1.   Idea intuitiva.
2.   El SVM lineal e impkementaci√≥n 'from scratch'.
3.   SVM para datos no lineales. El kernel trick.
4.   Comparaci√≥n de los distintos algoritmos.
5.   Conclusiones.




## Idea intuitiva

Imaginemos que estamos en un zool√≥gico y tenemos dos tipos de animales: leones y jirafas. Queremos encontrar una forma de separarlos en diferentes grupos utilizando el SVM lineal.

El SVM lineal nos ayuda a trazar una l√≠nea imaginaria en el suelo del zool√≥gico para separar los leones de las jirafas. ¬øC√≥mo lo hace? Imagina que tienes una cuerda larga . Tu objetivo es colocarla en el suelo de tal manera que los leones est√©n en un lado de la cuerda y las jirafas est√©n en el otro lado.

Para hacer esto, primero necesitamos observar algunas caracter√≠sticas de los animales. Por ejemplo, podr√≠amos medir la altura de los animales y la longitud de sus patas. Estas caracter√≠sticas nos ayudar√°n a distinguir entre leones y jirafas.

Ahora, supongamos que hemos recolectado los datos y hemos creado un gr√°fico con la altura en el eje vertical y la longitud de las patas en el eje horizontal. En el gr√°fico, cada le√≥n y cada jirafa se representa como un punto.

El SVM lineal tratar√° de encontrar la mejor l√≠nea recta que pueda separar los puntos de los leones y las jirafas. Querr√° dibujar esa l√≠nea de tal manera que los leones queden en un lado y las jirafas queden en el otro.

Pero hay algo especial sobre esta l√≠nea. El SVM lineal tambi√©n se asegurar√° de que la l√≠nea est√© lo m√°s alejada posible de los animales m√°s cercanos a ella. Esto significa que habr√° una "zona segura" a cada lado de la l√≠nea, donde no habr√° animales.

Entonces, en nuestro ejemplo del zool√≥gico, el SVM lineal buscar√° la mejor l√≠nea recta que separe a los leones y las jirafas utilizando las caracter√≠sticas de altura y longitud de las patas. Dibujar√° una l√≠nea en el suelo y se asegurar√° de que los leones est√©n en un lado y las jirafas est√©n en el otro. Tambi√©n se asegurar√° de que la l√≠nea est√© lo m√°s alejada posible de los animales m√°s cercanos.



# SVM Lineal y el Margen M√°ximo

En el campo del aprendizaje autom√°tico, las M√°quinas de Vectores de Soporte (SVM, por sus siglas en ingl√©s) son un poderoso algoritmo de clasificaci√≥n. En particular, el SVM lineal se utiliza para separar dos clases diferentes de objetos en un espacio dimensional mediante un hiperplano lineal. A continuaci√≥n exploraremos en detalle el concepto del SVM lineal y su enfoque de maximizaci√≥n del margen.

## La Funci√≥n del Hiperplano

En el caso del SVM lineal, el objetivo es encontrar un hiperplano que logre separar √≥ptimamente las clases de objetos. Matem√°ticamente, el hiperplano se define como:

`f(x) = w¬∑ x + b`


donde:

- `f(x)` es la funci√≥n que clasifica los objetos.
- `w` es el vector de pesos.
- `x` es el vector de caracter√≠sticas del objeto.
- `b` es el sesgo (tambi√©n conocido como t√©rmino de sesgo o t√©rmino de intercepci√≥n).

El hiperplano divide el espacio dimensional en dos regiones, una para cada clase. Los objetos se asignan a una clase en funci√≥n de qu√© lado del hiperplano se encuentren.

## Maximizando el Margen

En el contexto del SVM lineal, maximizar el margen es una parte fundamental para lograr una buena separaci√≥n entre las clases. El margen se define como la distancia m√°s corta entre el hiperplano de separaci√≥n y los vectores de soporte, que son los objetos m√°s cercanos al hiperplano y pertenecen a diferentes clases.

La f√≥rmula para calcular el margen m√°ximo es `Margen m√°ximo = 2 / ||w||`, donde `||w||` representa la norma euclidiana del vector de pesos `w`. La norma euclidiana se calcula como la ra√≠z cuadrada de la suma de los cuadrados de los elementos del vector. La magnitud de `||w||` est√° inversamente relacionada con el margen: cuanto mayor sea el valor de `||w||`, m√°s estrecho ser√° el margen, y viceversa.

Para maximizar el margen, es necesario encontrar los vectores de soporte m√°s cercanos al hiperplano. Estos vectores de soporte influyen en la posici√≥n y orientaci√≥n del hiperplano de separaci√≥n. Al ajustar el hiperplano de manera que la distancia a los vectores de soporte sea m√°xima, se logra un margen √≥ptimo.

Los vectores de soporte desempe√±an un papel crucial en el SVM lineal, ya que definen el l√≠mite de decisi√≥n entre las clases. Son los objetos que se encuentran m√°s pr√≥ximos al hiperplano y, por lo tanto, son los m√°s dif√≠ciles de clasificar correctamente. Al maximizar el margen, se busca minimizar el riesgo de clasificar incorrectamente los vectores de soporte.

El concepto de maximizaci√≥n del margen en el SVM lineal es importante porque proporciona una buena generalizaci√≥n y capacidad de clasificaci√≥n para nuevos datos. Al aumentar el margen, se reduce la probabilidad de sobreajuste y se mejora la capacidad del modelo para separar las clases de manera m√°s robusta.

En resumen, maximizar el margen en el SVM lineal implica encontrar los vectores de soporte m√°s cercanos al hiperplano y ajustar el hiperplano para que su distancia a estos vectores sea m√°xima. Esto se logra calculando la norma euclidiana del vector de pesos y utilizando la f√≥rmula `Margen m√°ximo = 2 / ||w||`. Al maximizar el margen, se mejora la capacidad del modelo para clasificar correctamente los nuevos datos y se reduce el riesgo de sobreajuste.

![Imagen de un ejemplo gr√°fico de SVM lineal](https://3.bp.blogspot.com/-Zt5ab9bdQ64/UxsBSEH-3_I/AAAAAAAADRE/caQodexaP2c/s1600/36.png)

## Funci√≥n de P√©rdida y Optimizaci√≥n

En el SVM lineal, se utiliza la funci√≥n de p√©rdida conocida como Bisagra (Hinge) para medir el error de clasificaci√≥n. La funci√≥n de p√©rdida se define como:

`L(y, f(x)) = max(0, 1 - y ¬∑ f(x))`


donde `y` es la etiqueta de clase verdadera `(-1 o 1)` y `f(x)` es la salida del hiperplano.

El objetivo de entrenar el SVM lineal es minimizar la funci√≥n de p√©rdida, al tiempo que se busca maximizar el margen. Esto se logra a trav√©s de la optimizaci√≥n de un problema de programaci√≥n cuadr√°tica.


## Implementaci√≥n from scratch:

A continuaci√≥n se realiza una implementaci√≥n del algoritmo 'from scratch'. Esto quiere decir que no se usa ninguna librer√≠a donde ya est√© implementado el algoritmo como por ejemplo scikit-learn, si no que se implementa de 0 usando numpy.

El proceso de maximizaci√≥n del margen en SVM lineal implica encontrar los par√°metros √≥ptimos del hiperplano de separaci√≥n. A continuaci√≥n se presenta un resumen del algoritmo en pseudoc√≥digo para maximizar el margen:

1. Comenzar con un valor inicial grande para `w`, por ejemplo, `(w0, w0)`. Este valor se disminuir√° m√°s adelante.
2. Seleccionar un tama√±o de paso como `w0 * 0.1`.
3. Establecer un valor peque√±o para `b`, que se aumentar√° m√°s adelante. El rango de `b` ser√° `(-b0 < b < +b0)`, con un paso de incremento de `b_multiple`. Es importante seleccionar sabiamente el valor de `b0` para evitar un costo computacional excesivo.
4. Se realizar√° el siguiente chequeo para cada punto `xi` en el conjunto de datos:
    - Comprobar para todas las transformaciones de `w`, como `(w0, w0), (-w0, w0), (w0, -w0), (-w0, -w0)`.
    - Si `yi(xi.w + b)` no es mayor o igual a 1 para todos los puntos, detener el proceso.
    - De lo contrario, calcular `|w|` y agregarlo a un diccionario como clave, y `(w, b)` como valor.
    - Si `w <= 0`, se ha completado el paso actual y se pasa al siguiente paso.
    - De lo contrario, disminuir `w` como `(w0 - step, w0 - step)` y continuar con el paso 3.
5. Repetir este proceso hasta que el paso se convierta en `w0 * 0.001`, ya que m√°s adelante ser√° un punto costoso:
    - Reducir el tama√±o del paso como `step = step * 0.1`.
    - Volver al paso 3.
6. Seleccionar `(w, b)` que tenga la menor `|w|` del diccionario como resultado final del proceso de maximizaci√≥n del margen.

Este algoritmo permite encontrar los par√°metros de `w` y `b` que maximizan el margen en SVM lineal. Al maximizar el margen, se mejora la capacidad de clasificaci√≥n del modelo y se reduce el riesgo de sobreajuste. Es importante ajustar adecuadamente los valores iniciales y los tama√±os de paso para obtener resultados √≥ptimos.

```python3

%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
import numpy as np

```

```python3
class SVM(object):
    def __init__(self, visualization=True):
        self.visualization = visualization
        self.colors = {1: 'r', -1: 'b'}
        if self.visualization:
            self.fig = plt.figure()
            self.ax = self.fig.add_subplot(1, 1, 1)

    def fit(self, data):
        self.data = data
        opt_dict = {}
        transforms = [[1, 1], [-1, 1], [-1, -1], [1, -1]]
        all_data = np.array([])

        for yi in self.data:
            all_data = np.append(all_data, self.data[yi])

        self.max_feature_value = max(all_data)
        self.min_feature_value = min(all_data)
        all_data = None

        step_sizes = [self.max_feature_value * 0.1, self.max_feature_value * 0.01, self.max_feature_value * 0.001]
        b_range_multiple = 5
        b_multiple = 5
        latest_optimum = self.max_feature_value * 10

        for step in step_sizes:
            w = np.array([latest_optimum, latest_optimum])
            optimized = False

            while not optimized:
                for b in np.arange(-1 * self.max_feature_value * b_range_multiple,
                                   self.max_feature_value * b_range_multiple,
                                   step * b_multiple):
                    for transformation in transforms:
                        w_t = w * transformation
                        found_option = True

                        for i in self.data:
                            for xi in self.data[i]:
                                yi = i
                                if not yi * (np.dot(w_t, xi) + b) >= 1:
                                    found_option = False

                        if found_option:
                            opt_dict[np.linalg.norm(w_t)] = [w_t, b]

                if w[0] < 0:
                    optimized = True
                else:
                    w = w - step

            norms = sorted([n for n in opt_dict])
            opt_choice = opt_dict[norms[0]]

            self.w = opt_choice[0]
            self.b = opt_choice[1]
            latest_optimum = opt_choice[0][0] + step * 2

    def visualize(self):
        [[self.ax.scatter(x[0], x[1], s=100, c=self.colors[i]) for x in data_dict[i]] for i in data_dict]

        def hyperplane(x, w, b, v):
            return (-w[0] * x - b + v) / w[1]

        hyp_x_min = self.min_feature_value * 0.9
        hyp_x_max = self.max_feature_value * 1.1

        pav1 = hyperplane(hyp_x_min, self.w, self.b, 1)
        pav2 = hyperplane(hyp_x_max, self.w, self.b, 1)
        self.ax.plot([hyp_x_min, hyp_x_max], [pav1, pav2], 'k')

        nav1 = hyperplane(hyp_x_min, self.w, self.b, -1)
        nav2 = hyperplane(hyp_x_max, self.w, self.b, -1)
        self.ax.plot([hyp_x_min, hyp_x_max], [nav1, nav2], 'k')

        db1 = hyperplane(hyp_x_min, self.w, self.b, 0)
        db2 = hyperplane(hyp_x_max, self.w, self.b, 0)
        self.ax.plot([hyp_x_min, hyp_x_max], [db1, db2], 'y--')
```

```python3
data_dict = {-1:np.array([[1,7],[2,8],[3,8]]),1:np.array([[5,1],[6,-1],[7,3]])}
svm = SVM() 
svm.fit(data=data_dict)
svm.visualize()
```

![Resultado script](https://github.com/boresmol/boresmol.github.io/blob/master/images/descargar.png?raw=true)

Como vemos, este es un ejemplo muy simple. 3 puntos en cada clase perfectamente separables por una recta. Tristemente, los datos reales suelen ser de naturaleza no lineal, por lo que el algoritmo descrito e implementado anteriormente no nos sirve para separar clases. Para solucionar este problema, tenemos que hacer uso de lo que se conoce como 'kernel trick' que explicaremos a continuaci√≥n.

# SVM para datos no lineales. El kernel trick.

En casos donde los datos no pueden ser separados perfectamente por un hiperplano lineal, se utiliza una t√©cnica llamada "kernel trick". Esta t√©cnica permite mapear los datos a un espacio de caracter√≠sticas de mayor dimensi√≥n donde los objetos se pueden separar de manera lineal. Se introduce una funci√≥n de kernel que realiza la transformaci√≥n de los datos. El kernel m√°s utilizado es el kernel radial, pero, ¬øqu√© es esto del kernel?

## El kernel en SVM

## Funciones de Kernel en SVM

Las funciones de kernel son una herramienta muy poderosa para explorar espacios de alta dimensi√≥n. Nos permiten realizar discriminantes lineales en variedades no lineales, lo que puede llevar a una mayor precisi√≥n y robustez que los modelos lineales tradicionales por s√≠ solos.

La funci√≥n de kernel es simplemente una funci√≥n matem√°tica que convierte un espacio de entrada de baja dimensi√≥n en un espacio de dimensi√≥n superior. Esto se logra mediante la asignaci√≥n de los datos a un nuevo espacio de caracter√≠sticas. En este espacio, los datos ser√°n linealmente separables. Esto significa que se puede utilizar una m√°quina de vectores de soporte para encontrar un hiperplano que separe los datos.

Por ejemplo, si la entrada ùë• es bidimensional, la funci√≥n de kernel la mapear√° a un espacio tridimensional. En este espacio, los datos ser√°n linealmente separables.

![Mapeo kernel](https://github.com/boresmol/boresmol.github.io/blob/master/images/descargar%20(1).png?raw=true)

Adem√°s, las funciones de kernel proporcionan m√°s caracter√≠sticas que otros algoritmos como redes neuronales o conjuntos de √°rboles en algunos tipos de problemas que involucran reconocimiento de escritura a mano, detecci√≥n de caras, etc., porque extraen propiedades intr√≠nsecas de los puntos de datos a trav√©s de una funci√≥n de kernel.

## El kernel radial

RBF, que significa Funci√≥n de Base Radial (por sus siglas en ingl√©s, Radial Basis Function), es un kernel muy poderoso utilizado en SVM. A diferencia de los kernels lineales o polin√≥micos, RBF es m√°s complejo y eficiente al mismo tiempo, ya que puede combinar m√∫ltiples kernels polin√≥micos de diferentes grados varias veces para proyectar los datos no linealmente separables en un espacio de dimensionalidad superior, de modo que puedan ser separables utilizando un hiperplano.

![no lineal](https://github.com/boresmol/boresmol.github.io/blob/master/images/descargar%20(2).png?raw=true)

El kernel RBF funciona mapeando los datos a un espacio de alta dimensi√≥n mediante la b√∫squeda de los productos escalares y los cuadrados de todas las caracter√≠sticas en el conjunto de datos, y luego realizando la clasificaci√≥n utilizando la idea b√°sica del SVM lineal. Para proyectar los datos en un espacio de dimensionalidad superior, el kernel RBF utiliza la llamada funci√≥n de base radial, que puede escribirse como:

![formula1](https://github.com/boresmol/boresmol.github.io/blob/master/images/formula1.png?raw=true)

Aqu√≠, `||X1 - X2||^2` se conoce como la Distancia Euclidiana al Cuadrado y `œÉ` es un par√°metro libre que se puede utilizar para ajustar la ecuaci√≥n.

Cuando se introduce un nuevo par√°metro `‚ÑΩ = 1 / (2œÉ^2)`, la ecuaci√≥n ser√°:

![formula2](https://github.com/boresmol/boresmol.github.io/blob/master/images/formula2.png?raw=true)

La ecuaci√≥n es realmente simple : la Distancia Euclidiana al Cuadrado se multiplica por el par√°metro gamma y luego se encuentra el exponente de todo el resultado. Esta ecuaci√≥n puede encontrar los productos internos transformados para mapear los datos a dimensiones superiores directamente sin necesidad de transformar todo el conjunto de datos, lo que conduce a una ineficiencia. Y es por esto que se conoce como la funci√≥n de kernel RBF (Radial Basis Function).

La gr√°fica de distribuci√≥n del RBF Kernel se ver√° as√≠:

![distribucion](https://github.com/boresmol/boresmol.github.io/blob/master/images/distribucion.png?raw=true)

Como puedes ver, la gr√°fica de distribuci√≥n del RBF kernel se asemeja a la curva de la distribuci√≥n gaussiana, que es conocida como una curva en forma de campana. Por lo tanto, el kernel RBF tambi√©n se conoce como el kernel de base radial gaussiana.

El kernel RBF se utiliza de manera muy popular con algoritmos como K-Nearest Neighbors (K-Vecinos m√°s Cercanos) y Support Vector Machines (M√°quinas de Vectores de Soporte).

Por √∫ltimo, esta visualizaci√≥n final resume muy bien todo lo explicado en esta secci√≥n del post:

![kernel](https://github.com/boresmol/boresmol.github.io/blob/master/images/kernel.png?raw=true)

# Comparaci√≥n de algoritmos

A continuaci√≥n vamos a hacer una peque√±a experimentaci√≥n para acabar de entender la importancia del kernel trick en datos no lineales:

1. Generaremos un conjunto de datos artificial no lineal de dos clases
2. Crearemos un SVM Lineal para tratar de separar las clases
3. Usaremos el kernel trick para tratar de separar las clases
4. Compararemos el rendimiento

```python3

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_circles

X, y = make_circles(n_samples=500, noise=0.06, random_state=42)

df = pd.DataFrame(dict(x1=X[:, 0], x2=X[:, 1], y=y))

```

```python3

colors = {0:'blue', 1:'yellow'}
fig, ax = plt.subplots()
grouped = df.groupby('y')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color = colors[key])
plt.show()
```

![Resultado codigo](https://github.com/boresmol/boresmol.github.io/blob/master/images/resultado_codigo.png?raw=true)

```python3

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

clf = SVC(kernel="linear")

clf.fit(X, y)

pred = clf.predict(X)

print("El accuracy del modelo lineal es: ",accuracy_score(pred, y))
```
**El accuracy del modelo lineal es: 0.496**

```python3

# Definimos el kernel radial con la f√≥rmula antes explicada

def RBF(X, gamma):

    # Free parameter gamma
    if gamma == None:
        gamma = 1.0/X.shape[1]

    # RBF kernel Equation
    K = np.exp(-gamma * np.sum((X - X[:,np.newaxis])**2, axis = -1))

    return K

```

```python3
# Aplicamos el kernel al dataset

X = RBF(X, gamma=None)
```

```python3
# Alicamos el kernel lineal a un dataset al que se le ha aplicado el kernel trick

clf = SVC(kernel="linear")

clf.fit(X, y)

pred = clf.predict(X)

print("El accuracy del SVM con kernel radial es: ",accuracy_score(pred, y))
```
**El accuracy del SVM con kernel radial es:  0.94**


Como podemos ver, el accuracy del SVM con kernel radial es significativamente mayor al SVM lineal simple, lo que demuestra la suma importancia del kernel a la hora de enfrentarnos a datos no lineales.

# Conclusiones
SVM es un algoritmo de machine learning altamente efectivo y vers√°til. De hecho, aunque actualmente no es el estado del arte, ha sido muy utilizado durante d√©cadas en el campo de la visi√≥n por computador, m√°s concretamente en el reconocimiento de d√≠gitos manuscritos y de rostros.

Las ventajas de las m√°quinas de vectores de soporte son las siguientes:

- Efectivas en espacios de alta dimensionalidad.
- Siguen siendo efectivas en casos donde el n√∫mero de dimensiones es mayor que el n√∫mero de muestras.
- Utilizan un subconjunto de puntos de entrenamiento en la funci√≥n de decisi√≥n (llamados vectores de soporte), por lo que tambi√©n son eficientes en memoria.
- Vers√°tiles: se pueden especificar diferentes funciones de kernel para la funci√≥n de decisi√≥n. Se proporcionan kernels comunes, pero tambi√©n es posible especificar kernels personalizados.

Las desventajas de las m√°quinas de vectores de soporte incluyen:

- Si el n√∫mero de caracter√≠sticas es mucho mayor que el n√∫mero de muestras, es crucial evitar el sobreajuste al elegir funciones de kernel y t√©rminos de regularizaci√≥n.
- Las SVM no proporcionan directamente estimaciones de probabilidad. Estas se calculan utilizando una costosa validaci√≥n cruzada de cinco pliegues (ver Puntuaciones y probabilidades).
- Las M√°quinas de Vectores de Soporte son herramientas poderosas, pero sus requisitos computacionales y de almacenamiento aumentan r√°pidamente con el n√∫mero de vectores de entrenamiento. El n√∫cleo de una SVM es un problema de programaci√≥n cuadr√°tica.


Mencionar que existen versiones de [SVM para regresi√≥n](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) como para [clasificaci√≥n multiclase](https://scikit-learn.org/stable/modules/svm.html#classification)



